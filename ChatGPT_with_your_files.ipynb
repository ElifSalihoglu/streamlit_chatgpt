{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElifSalihoglu/streamlit_chatgpt/blob/main/ChatGPT_with_your_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirement"
      ],
      "metadata": {
        "id": "0l3MD3312Itl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip uninstall Pillow\n",
        "# !pip install --upgrade Pillow\n",
        "# import PIL\n",
        "# print(PIL.__version__)"
      ],
      "metadata": {
        "id": "zxvS6Afa_RI7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.27.1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qVzm1f2s6_U",
        "outputId": "2cfb1294-11dc-4c30-9ccc-fe904331f04a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests==2.27.1\n",
            "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1) (3.4)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.4.4 requires requests>=2.28, but you have requests 2.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6WI5qfkUe0Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c03035f-9665-4009-91f0-092097869ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install chromadb -q\n",
        "!pip install tiktoken -q\n",
        "!pip install pypdf -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install gradio -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "TgkUctQX2Pbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUTbnXkHe6Rv"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0,model_name=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Da3yPddNjxY"
      },
      "source": [
        "# Load your data into Reports folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoir5Y5xvU_g",
        "outputId": "18db7184-3f95-4972-ac69-ab45880fd6d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZPUMNCpv9FO",
        "outputId": "990088bf-b614-47cb-e4af-31dacc4305cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSxialfUTh9b",
        "outputId": "eb5f6289-bc4d-4b70-aeea-48c8b6edb931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents: 2\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "pdf_loader = DirectoryLoader('./Reports/', glob=\"**/*.pdf\")\n",
        "txt_loader = DirectoryLoader('./Reports/', glob=\"**/*.txt\")\n",
        "word_loader = DirectoryLoader('./Reports/', glob=\"**/*.docx\")\n",
        "csv_loader = DirectoryLoader('./Reports/', glob=\"**/*.csv\")\n",
        "\n",
        "loaders = [pdf_loader, txt_loader, word_loader, csv_loader]\n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "print(f\"Total number of documents: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34HYlsekNona"
      },
      "source": [
        "# Chunk the data, turn into Embeddings and save to VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5aUybd6KhN7F"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptdb1wVuNzom"
      },
      "source": [
        "# Calling the Langchain's QA chain with Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tEBWBa1nhQwV"
      },
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4okS40zMrLIL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKmAxuY9rqxK"
      },
      "source": [
        "# Gradio Chat UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAJVCq7Sly5Y"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "# Define chat interface\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    chat_history = []\n",
        "\n",
        "    def user(query, chat_history):\n",
        "        print(\"User query:\", query)\n",
        "        print(\"Chat history:\", chat_history)\n",
        "\n",
        "        # Convert chat history to list of tuples\n",
        "        chat_history_tuples = []\n",
        "        for message in chat_history:\n",
        "            chat_history_tuples.append((message[0], message[1]))\n",
        "\n",
        "        # Get result from QA chain\n",
        "        result = qa({\"question\": query, \"chat_history\": chat_history_tuples})\n",
        "\n",
        "        # Append user message and response to chat history\n",
        "        chat_history.append((query, result[\"answer\"]))\n",
        "        print(\"Updated chat history:\", chat_history)\n",
        "\n",
        "        return gr.update(value=\"\"), chat_history\n",
        "\n",
        "\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}